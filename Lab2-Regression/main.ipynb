{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.__version__[0] == \"2\":\n",
    "    from torch import mps\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "targets = raw_df.values[1::2, 2]\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',\n",
    "                 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "print(data.shape)\n",
    "print(targets.shape)\n",
    "print(data[0])\n",
    "print(targets[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "class Regression(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(in_features, 1))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.constant_(self.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.float()\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "    def getWeight(self):\n",
    "        return torch.cat([self.weight, self.bias.unsqueeze(0)], dim=0)\n",
    "\n",
    "\n",
    "def simpleRegressionLoss(target, predict, matrix):\n",
    "    return torch.mean(0.5*(target-predict)**2)\n",
    "\n",
    "\n",
    "def lassoRegressionLoss(target, predict, matrix):\n",
    "    return torch.mean(0.5*(target-predict)**2 + alpha*torch.abs(torch.linalg.norm(matrix)))\n",
    "\n",
    "\n",
    "def ridgeRegressionLoss(target, predict, matrix):\n",
    "    return torch.mean(0.5*(target-predict)**2 + alpha*torch.linalg.norm(matrix)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, lossFunc, device=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if device is not None:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = lossFunc(output, target, model.getWeight())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1) % 5 == 0 or batch_idx == 13:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, test_loader, lossFunc, device=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if device is not None:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            print(output)\n",
    "            y_pred.append(output.item())\n",
    "            y_true.append(target)\n",
    "            test_loss += lossFunc(output, target, model.getWeight()).item()\n",
    "            pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(test_loss))\n",
    "    print(mean_squared_error(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonDataset=BostonDataset(data,targets)\n",
    "train_size = int(0.8 * len(bostonDataset))\n",
    "test_size = len(bostonDataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(bostonDataset, [train_size, test_size])\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/404 (57%)]\tLoss: 47535.764700\n",
      "Train Epoch: 2 [256/404 (57%)]\tLoss: 31638.740001\n",
      "Train Epoch: 3 [256/404 (57%)]\tLoss: 17766.679586\n",
      "Train Epoch: 4 [256/404 (57%)]\tLoss: 8895.348738\n",
      "Train Epoch: 5 [256/404 (57%)]\tLoss: 4123.566894\n",
      "Train Epoch: 6 [256/404 (57%)]\tLoss: 1433.826360\n",
      "Train Epoch: 7 [256/404 (57%)]\tLoss: 359.844826\n",
      "Train Epoch: 8 [256/404 (57%)]\tLoss: 156.063619\n",
      "Train Epoch: 9 [256/404 (57%)]\tLoss: 125.341901\n",
      "Train Epoch: 10 [256/404 (57%)]\tLoss: 176.450402\n",
      "Train Epoch: 11 [256/404 (57%)]\tLoss: 158.213762\n",
      "Train Epoch: 12 [256/404 (57%)]\tLoss: 93.856853\n",
      "Train Epoch: 13 [256/404 (57%)]\tLoss: 141.661455\n",
      "Train Epoch: 14 [256/404 (57%)]\tLoss: 127.370994\n",
      "Train Epoch: 15 [256/404 (57%)]\tLoss: 116.320284\n",
      "Train Epoch: 16 [256/404 (57%)]\tLoss: 101.301133\n",
      "Train Epoch: 17 [256/404 (57%)]\tLoss: 114.950437\n",
      "Train Epoch: 18 [256/404 (57%)]\tLoss: 100.260789\n",
      "Train Epoch: 19 [256/404 (57%)]\tLoss: 137.393383\n",
      "Train Epoch: 20 [256/404 (57%)]\tLoss: 117.913015\n",
      "Train Epoch: 21 [256/404 (57%)]\tLoss: 93.597445\n",
      "Train Epoch: 22 [256/404 (57%)]\tLoss: 93.749512\n",
      "Train Epoch: 23 [256/404 (57%)]\tLoss: 145.056184\n",
      "Train Epoch: 24 [256/404 (57%)]\tLoss: 101.658736\n",
      "Train Epoch: 25 [256/404 (57%)]\tLoss: 74.653392\n",
      "Train Epoch: 26 [256/404 (57%)]\tLoss: 98.124902\n",
      "Train Epoch: 27 [256/404 (57%)]\tLoss: 107.766484\n",
      "Train Epoch: 28 [256/404 (57%)]\tLoss: 89.677338\n",
      "Train Epoch: 29 [256/404 (57%)]\tLoss: 86.376074\n",
      "Train Epoch: 30 [256/404 (57%)]\tLoss: 97.039494\n",
      "Train Epoch: 31 [256/404 (57%)]\tLoss: 102.638727\n",
      "Train Epoch: 32 [256/404 (57%)]\tLoss: 87.911408\n",
      "Train Epoch: 33 [256/404 (57%)]\tLoss: 95.207838\n",
      "Train Epoch: 34 [256/404 (57%)]\tLoss: 85.225393\n",
      "Train Epoch: 35 [256/404 (57%)]\tLoss: 89.916316\n",
      "Train Epoch: 36 [256/404 (57%)]\tLoss: 94.490295\n",
      "Train Epoch: 37 [256/404 (57%)]\tLoss: 81.513726\n",
      "Train Epoch: 38 [256/404 (57%)]\tLoss: 92.722884\n",
      "Train Epoch: 39 [256/404 (57%)]\tLoss: 86.053662\n",
      "Train Epoch: 40 [256/404 (57%)]\tLoss: 97.335075\n",
      "Train Epoch: 41 [256/404 (57%)]\tLoss: 85.859714\n",
      "Train Epoch: 42 [256/404 (57%)]\tLoss: 76.057932\n",
      "Train Epoch: 43 [256/404 (57%)]\tLoss: 83.736549\n",
      "Train Epoch: 44 [256/404 (57%)]\tLoss: 73.446916\n",
      "Train Epoch: 45 [256/404 (57%)]\tLoss: 84.508253\n",
      "Train Epoch: 46 [256/404 (57%)]\tLoss: 95.986272\n",
      "Train Epoch: 47 [256/404 (57%)]\tLoss: 84.171731\n",
      "Train Epoch: 48 [256/404 (57%)]\tLoss: 78.442064\n",
      "Train Epoch: 49 [256/404 (57%)]\tLoss: 98.332642\n",
      "Train Epoch: 50 [256/404 (57%)]\tLoss: 92.398992\n",
      "Train Epoch: 51 [256/404 (57%)]\tLoss: 93.233107\n",
      "Train Epoch: 52 [256/404 (57%)]\tLoss: 78.502674\n",
      "Train Epoch: 53 [256/404 (57%)]\tLoss: 64.948367\n",
      "Train Epoch: 54 [256/404 (57%)]\tLoss: 91.433359\n",
      "Train Epoch: 55 [256/404 (57%)]\tLoss: 79.307132\n",
      "Train Epoch: 56 [256/404 (57%)]\tLoss: 68.924374\n",
      "Train Epoch: 57 [256/404 (57%)]\tLoss: 87.823310\n",
      "Train Epoch: 58 [256/404 (57%)]\tLoss: 66.260838\n",
      "Train Epoch: 59 [256/404 (57%)]\tLoss: 73.618732\n",
      "Train Epoch: 60 [256/404 (57%)]\tLoss: 73.658022\n",
      "Train Epoch: 61 [256/404 (57%)]\tLoss: 62.056248\n",
      "Train Epoch: 62 [256/404 (57%)]\tLoss: 78.616878\n",
      "Train Epoch: 63 [256/404 (57%)]\tLoss: 85.157030\n",
      "Train Epoch: 64 [256/404 (57%)]\tLoss: 91.736272\n",
      "Train Epoch: 65 [256/404 (57%)]\tLoss: 101.624342\n",
      "Train Epoch: 66 [256/404 (57%)]\tLoss: 81.002271\n",
      "Train Epoch: 67 [256/404 (57%)]\tLoss: 67.303909\n",
      "Train Epoch: 68 [256/404 (57%)]\tLoss: 76.851170\n",
      "Train Epoch: 69 [256/404 (57%)]\tLoss: 63.875457\n",
      "Train Epoch: 70 [256/404 (57%)]\tLoss: 62.809052\n",
      "Train Epoch: 71 [256/404 (57%)]\tLoss: 78.142047\n",
      "Train Epoch: 72 [256/404 (57%)]\tLoss: 74.097158\n",
      "Train Epoch: 73 [256/404 (57%)]\tLoss: 70.749549\n",
      "Train Epoch: 74 [256/404 (57%)]\tLoss: 85.440522\n",
      "Train Epoch: 75 [256/404 (57%)]\tLoss: 77.786642\n",
      "Train Epoch: 76 [256/404 (57%)]\tLoss: 65.182616\n",
      "Train Epoch: 77 [256/404 (57%)]\tLoss: 87.129510\n",
      "Train Epoch: 78 [256/404 (57%)]\tLoss: 88.919263\n",
      "Train Epoch: 79 [256/404 (57%)]\tLoss: 59.088076\n",
      "Train Epoch: 80 [256/404 (57%)]\tLoss: 83.659872\n",
      "Train Epoch: 81 [256/404 (57%)]\tLoss: 53.901176\n",
      "Train Epoch: 82 [256/404 (57%)]\tLoss: 91.195513\n",
      "Train Epoch: 83 [256/404 (57%)]\tLoss: 77.489815\n",
      "Train Epoch: 84 [256/404 (57%)]\tLoss: 77.277038\n",
      "Train Epoch: 85 [256/404 (57%)]\tLoss: 60.654201\n",
      "Train Epoch: 86 [256/404 (57%)]\tLoss: 60.587466\n",
      "Train Epoch: 87 [256/404 (57%)]\tLoss: 82.570992\n",
      "Train Epoch: 88 [256/404 (57%)]\tLoss: 61.888401\n",
      "Train Epoch: 89 [256/404 (57%)]\tLoss: 66.302806\n",
      "Train Epoch: 90 [256/404 (57%)]\tLoss: 65.587744\n",
      "Train Epoch: 91 [256/404 (57%)]\tLoss: 51.652763\n",
      "Train Epoch: 92 [256/404 (57%)]\tLoss: 75.172142\n",
      "Train Epoch: 93 [256/404 (57%)]\tLoss: 67.545592\n",
      "Train Epoch: 94 [256/404 (57%)]\tLoss: 65.728655\n",
      "Train Epoch: 95 [256/404 (57%)]\tLoss: 67.588915\n",
      "Train Epoch: 96 [256/404 (57%)]\tLoss: 64.272203\n",
      "Train Epoch: 97 [256/404 (57%)]\tLoss: 69.589977\n",
      "Train Epoch: 98 [256/404 (57%)]\tLoss: 55.831236\n",
      "Train Epoch: 99 [256/404 (57%)]\tLoss: 75.112289\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 64 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiaruiye/FDU/专业课程/选修课程/机器学习/Labs/Lab2-Regression/main.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(model, train_loader, optimizer, epoch, simpleRegressionLoss)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test(model, test_loader, simpleRegressionLoss)\n",
      "\u001b[1;32m/Users/jiaruiye/FDU/专业课程/选修课程/机器学习/Labs/Lab2-Regression/main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m y_pred\u001b[39m.\u001b[39mappend(output\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y_true\u001b[39m.\u001b[39mappend(target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiaruiye/FDU/%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B/%E9%80%89%E4%BF%AE%E8%AF%BE%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Labs/Lab2-Regression/main.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m lossFunc(output, target, model\u001b[39m.\u001b[39mgetWeight())\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 64 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "model=Regression(13)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "for epoch in range(1, 100):\n",
    "    train(model, train_loader, optimizer, epoch, simpleRegressionLoss)\n",
    "test(model, test_loader, simpleRegressionLoss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
